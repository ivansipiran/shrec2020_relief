from __future__ import print_function
import argparse
import torch
import torch.nn.parallel
import torch.utils.data
from torch.autograd import Variable
from pointnet.dataset import ShapeNetDataset
from pointnet.model import PointNetCls
import torch.nn.functional as F
import numpy as np
import time


parser = argparse.ArgumentParser()

parser.add_argument('--model', type=str, default = '',  help='model path')
parser.add_argument('--num_points', type=int, default=2500, help='input batch size')
parser.add_argument('--folder', type=str, default='', help='folder with input files')

opt = parser.parse_args()
print(opt)

taset = ShapeNetDataset(
#    root='shapenetcore_partanno_segmentation_benchmark_v0',
#    split='test',
#    classification=True,
#    npoints=opt.num_points,
#    data_augmentation=False)

#testdataloader = torch.utils.data.DataLoader(
#    test_dataset, batch_size=32, shuffle=True)

#print(len(test_dataset.classes))
classifier = PointNetCls(k=16, only_feature=True, feature_transform=True)
classifier.cuda()

classifier.load_state_dict(torch.load(opt.model))
classifier.eval()

#Models are in range [1,200]
for model in range(1, 201):
    desc = []
    start_time = time.time()
    #Each model has 200 patches
    for patch in range(200):
        filepath = opt.folder + '/' + str(model) + '_' + str(patch) + '.xyz'
        point_set = np.loadtxt(filepath).astype(np.float32)
        choice = np.random.choice(3000, opt.num_points, replace=False)
        point_set = point_set[choice, :]

        point_set = point_set - np.expand_dims(np.mean(point_set, axis=0), 0)
        dist = np.max(np.sqrt(np.sum(point_set**2, axis=1)), 0)
        point_set = point_set / dist

        point_set = torch.from_numpy(point_set)
        point_set = point_set.unsqueeze(0)
        point_set = point_set.transpose(2, 1)
        point_set = point_set.cuda()

        feature, _,_ = classifier(point_set)

        feature = feature.cpu().detach().numpy()
        desc.append(feature)

    desc = np.vstack(desc)
    print(desc.shape)
    outputfile = opt.folder + '/' + str(model) + '.desc'
    np.savetxt(outputfile, desc, fmt='%2.6f')

    print('Model {} processed in {} seconds'.format(model, time.time()-start_time))

#for i, data in enumerate(testdataloader, 0):
#    points, target = data
#    points, target = Variable(points), Variable(target[:, 0])
#    points = points.transpose(2, 1)
#    points, target = points.cuda(), target.cuda()
#    pred, _, _ = classifier(points)
#    loss = F.nll_loss(pred, target)

#    pred_choice = pred.data.max(1)[1]
#    correct = pred_choice.eq(target.data).cpu().sum()
#    print('i:%d  loss: %f accuracy: %f' % (i, loss.data.item(), correct / float(32)))
